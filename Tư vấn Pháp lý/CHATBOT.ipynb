{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adebdd3d-8f4a-4c37-85fa-1872f6a4face",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2docx\n",
      "  Downloading pdf2docx-0.5.8-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: python-docx in c:\\users\\daoth\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\daoth\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\daoth\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\daoth\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\daoth\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Collecting PyMuPDF>=1.19.0 (from pdf2docx)\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: fonttools>=4.24.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pdf2docx) (4.51.0)\n",
      "Collecting opencv-python-headless>=4.5 (from pdf2docx)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting fire>=0.3.0 (from pdf2docx)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from python-docx) (4.11.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Collecting termcolor (from fire>=0.3.0->pdf2docx)\n",
      "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Downloading pdf2docx-0.5.8-py3-none-any.whl (132 kB)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "   ---------------------------------------- 0.0/39.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/39.4 MB 7.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.1/39.4 MB 5.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.9/39.4 MB 4.8 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 4.2/39.4 MB 5.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 5.2/39.4 MB 5.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.6/39.4 MB 5.4 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 8.1/39.4 MB 5.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 9.4/39.4 MB 5.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 11.0/39.4 MB 6.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 12.1/39.4 MB 5.9 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 13.4/39.4 MB 5.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 14.7/39.4 MB 5.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 16.0/39.4 MB 6.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 16.5/39.4 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 17.8/39.4 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 19.1/39.4 MB 5.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 20.2/39.4 MB 5.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 21.2/39.4 MB 5.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 22.3/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 23.6/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 24.9/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 26.0/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 27.0/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 28.0/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 29.4/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 30.4/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 31.7/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 33.0/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 34.1/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 35.4/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.7/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.7/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.4/39.4 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/16.6 MB 6.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.4/16.6 MB 5.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/16.6 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.0/16.6 MB 5.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 6.0/16.6 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.3/16.6 MB 5.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.7/16.6 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.7/16.6 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.7/16.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.1/16.6 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.4/16.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.4/16.6 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.0/16.6 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114262 sha256=7e68845a37ef53ab63fe7f338e3c04fa89969962f19a9e1c6db57f7897e3f51a\n",
      "  Stored in directory: c:\\users\\daoth\\appdata\\local\\pip\\cache\\wheels\\9e\\5b\\45\\29f72e55d87a29426b04b3cfdf20325c079eb97ab74f59017d\n",
      "Successfully built fire\n",
      "Installing collected packages: termcolor, PyMuPDF, opencv-python-headless, fire, pdf2docx\n",
      "Successfully installed PyMuPDF-1.25.5 fire-0.7.0 opencv-python-headless-4.11.0.86 pdf2docx-0.5.8 termcolor-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pdf2docx PyPDF2 python-docx sentence-transformers numpy scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27795228-f529-4fc3-a8ba-7e48f3cc04ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Use pytorch device_name: cpu\n",
      "[INFO] Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã tải nội dung từ: data_crawl\\extracted_data.txt\n",
      "📜 Số đoạn văn bản pháp luật trích xuất: 9642\n",
      "\n",
      "🔎 Đang tạo embeddings cho văn bản pháp luật (quá trình có thể mất vài giây)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69829adc78cb40dc9ea3913bbee4791a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hoàn thành việc tạo embeddings.\n",
      "\n",
      "🤖 Chào mừng bạn đến với Chatbot tư vấn pháp luật!\n",
      "Gõ 'exit' hoặc 'thoát' để kết thúc cuộc trò chuyện.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Bạn:   Tại sao đất đai ở Việt Nam thuộc sở hữu toàn dân do Nhà nước quản lý?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c0f92a8e534b6b922b25b04aab4871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 Chatbot:\n",
      "📜 [Thông tin từ văn bản pháp luật]\n",
      "1. Nhà nước Cộng hòa xã hội chủ nghĩa Việt Nam là đại diện, thực hiện quyền của chủ sở hữu đối với tài sản thuộc sở hữu toàn dân.\n",
      "\n",
      "1. Nhà nước Cộng hòa xã hội chủ nghĩa Việt Nam là đại diện, thực hiện quyền của chủ sở hữu đối với tài sản thuộc sở hữu toàn dân.\n",
      "\n",
      "1. Nhà nước Cộng hòa xã hội chủ nghĩa Việt Nam, cơ quan nhà nước ở Trung ương, ở địa phương chịu trách nhiệm về nghĩa vụ dân sự của mình bằng tài sản mà mình là đại diện chủ sở hữu và thống nhất quản lý, trừ trường hợp tài sản đã được chuyển giao cho pháp nhân theo quy định tại khoản 2 Điều này.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Bạn:  hi Nhà nước thu hồi đất, người dân có quyền lợi gì? Chính sách bồi thường được áp dụng như thế nào?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86cad5823134c9badde740478a5dbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 Chatbot:\n",
      "📜 [Thông tin từ văn bản pháp luật]\n",
      "Nhà nước có trách nhiệm bồi thường thiệt hại và phục hồi danh dự, quyền lợi cho người bịgiữ trong trường hợp khẩn cấp, người bị bắt, bịtạm giữ, tạm giam, khởi tố, điều tra, truy tố, xét xử, thi hành án oan, trái pháp luật do cơ quan, người có thẩm quyền tiến hành tố tụng gây ra.\n",
      "\n",
      "Nhà nước có trách nhiệm bồi thường thiệt hại và phục hồi danh dự, quyền lợi cho người bịgiữ trong trường hợp khẩn cấp, người bị bắt, bịtạm giữ, tạm giam, khởi tố, điều tra, truy tố, xét xử, thi hành án oan, trái pháp luật do cơ quan, người có thẩm quyền tiến hành tố tụng gây ra.\n",
      "\n",
      "d) Được bồi thường thiệt hại, khôi phục danh dự, bảo đảm các quyền và lợi ích hợp pháp trong thời gian bảo vệ.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Bạn:   Nhà nước quy định giá đất như thế nào? Có những bất cập gì trong việc định giá đất?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f43d69a53f3439a86e5553989aeee04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 Chatbot:\n",
      "📜 [Thông tin từ văn bản pháp luật]\n",
      "Điều 325.Thế chấp quyền sử dụng đất mà không thế chấp tài sản gắn liền với đất\n",
      "\n",
      "Điều 325.Thế chấp quyền sử dụng đất mà không thế chấp tài sản gắn liền với đất\n",
      "\n",
      "Điều 326.Thế chấp tài sản gắn liền với đất mà không thế chấp quyền sử dụng đất\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Bạn:  thoát\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Chatbot: Cảm ơn bạn đã sử dụng dịch vụ!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "from pdf2docx import Converter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "###########################################\n",
    "# PHẦN 1: CHUYỂN ĐỔI VÀ TRÍCH XUẤT VĂN BẢN TỪ THƯ MỤC data_cawl\n",
    "###########################################\n",
    "\n",
    "DATA_FOLDER = \"data_crawl\"\n",
    "EXTRACTED_TEXT_FILE = os.path.join(DATA_FOLDER, \"extracted_data.txt\")\n",
    "\n",
    "def convert_pdf_to_docx(pdf_path, docx_path):\n",
    "    \"\"\"Chuyển đổi file PDF sang DOCX để trích xuất nội dung.\"\"\"\n",
    "    try:\n",
    "        cv = Converter(pdf_path)\n",
    "        cv.convert(docx_path, start=0, end=None)\n",
    "        cv.close()\n",
    "        print(f\"✅ Đã chuyển đổi PDF sang DOCX: {docx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi chuyển đổi PDF {pdf_path}: {e}\")\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    \"\"\"Trích xuất nội dung từ file DOCX.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        for para in doc.paragraphs:\n",
    "            if para.text.strip():\n",
    "                text += para.text.strip() + \"\\n\"\n",
    "        print(f\"✅ Đã trích xuất nội dung từ: {docx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi đọc DOCX {docx_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def process_all_documents(folder_path):\n",
    "    \"\"\"\n",
    "    Duyệt qua thư mục data_cawl, chuyển PDF sang DOCX (nếu cần)\n",
    "    và trích xuất nội dung từ các file DOCX.\n",
    "    \"\"\"\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Nếu là PDF thì chuyển sang DOCX trước\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            docx_path = file_path[:-4] + \".docx\"  # thay .pdf thành .docx\n",
    "            convert_pdf_to_docx(file_path, docx_path)\n",
    "            all_texts.append(extract_text_from_docx(docx_path))\n",
    "        elif filename.lower().endswith(\".docx\"):\n",
    "            all_texts.append(extract_text_from_docx(file_path))\n",
    "    return \"\\n\".join(all_texts)\n",
    "\n",
    "def save_extracted_text(text, filename=EXTRACTED_TEXT_FILE):\n",
    "    \"\"\"Lưu văn bản đã trích xuất vào file TXT.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"✅ Đã lưu văn bản trích xuất vào: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi lưu văn bản: {e}\")\n",
    "\n",
    "def load_extracted_text(filename=EXTRACTED_TEXT_FILE):\n",
    "    \"\"\"Tải văn bản đã trích xuất từ file TXT.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        print(f\"✅ Đã tải nội dung từ: {filename}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi tải văn bản từ file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Nếu file văn bản đã có, tải trực tiếp từ file; nếu không, xử lý gốc và lưu lại.\n",
    "if os.path.exists(EXTRACTED_TEXT_FILE):\n",
    "    combined_text = load_extracted_text()\n",
    "else:\n",
    "    combined_text = process_all_documents(DATA_FOLDER)\n",
    "    save_extracted_text(combined_text)\n",
    "\n",
    "###########################################\n",
    "# PHẦN 2: TIỀN XỬ LÝ VĂN BẢN – CHIA THÀNH CÁC ĐOẠN\n",
    "###########################################\n",
    "\n",
    "def split_into_paragraphs(text, min_length=50):\n",
    "    \"\"\"Chia văn bản thành các đoạn có ý nghĩa, bỏ qua các đoạn quá ngắn.\"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\") if len(p.strip()) >= min_length]\n",
    "    return paragraphs\n",
    "\n",
    "paragraphs = split_into_paragraphs(combined_text)\n",
    "print(f\"📜 Số đoạn văn bản pháp luật trích xuất: {len(paragraphs)}\")\n",
    "\n",
    "###########################################\n",
    "# PHẦN 3: TẠO EMBEDDINGS CHO CÁC ĐOẠN VĂN BẢN\n",
    "###########################################\n",
    "\n",
    "embed_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "print(\"\\n🔎 Đang tạo embeddings cho văn bản pháp luật (quá trình có thể mất vài giây)...\")\n",
    "paragraph_embeddings = embed_model.encode(paragraphs, convert_to_tensor=True)\n",
    "print(\"✅ Hoàn thành việc tạo embeddings.\")\n",
    "\n",
    "###########################################\n",
    "# PHẦN 4: TRÍCH XUẤT THÔNG TIN 'ĐIỀU'\n",
    "###########################################\n",
    "\n",
    "def extract_article_info(text):\n",
    "    \"\"\"\n",
    "    Sử dụng regex để tìm kiếm mẫu như \"Điều 15\" hoặc \"Điều15.1\" trong đoạn văn.\n",
    "    Trả về thông tin 'Điều' nếu tìm thấy, ngược lại trả về None.\n",
    "    \"\"\"\n",
    "    pattern = r'Điều\\s*\\d+([.,]\\d+)?'\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "###########################################\n",
    "# PHẦN 5: CẬP NHẬT CÂU HỎI – CÂU TRẢ LỜI VÀO FILE CSV (QA DATABASE)\n",
    "###########################################\n",
    "\n",
    "QA_DB_FILE = \"qa_database.csv\"\n",
    "\n",
    "def load_qa_database(filename):\n",
    "    \"\"\"Đọc dữ liệu từ file CSV nếu tồn tại.\"\"\"\n",
    "    data = []\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, mode='r', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                data.append(row)\n",
    "    return data\n",
    "\n",
    "def save_qa_database(filename, data):\n",
    "    \"\"\"Ghi danh sách các câu hỏi – câu trả lời vào file CSV.\"\"\"\n",
    "    fieldnames = ['question', 'answer', 'created_at', 'updated_at']\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def update_qa_database(filename, new_question, new_answer):\n",
    "    \"\"\"\n",
    "    Nếu câu hỏi đã có (so sánh không phân biệt hoa thường), cập nhật câu trả lời và thời gian;\n",
    "    Nếu chưa có thì thêm mới.\n",
    "    \"\"\"\n",
    "    data = load_qa_database(filename)\n",
    "    found = False\n",
    "    for entry in data:\n",
    "        if entry['question'].strip().lower() == new_question.strip().lower():\n",
    "            entry['answer'] = new_answer\n",
    "            entry['updated_at'] = datetime.now().isoformat()\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        now_str = datetime.now().isoformat()\n",
    "        data.append({\n",
    "            'question': new_question,\n",
    "            'answer': new_answer,\n",
    "            'created_at': now_str,\n",
    "            'updated_at': now_str\n",
    "        })\n",
    "    save_qa_database(filename, data)\n",
    "    return data\n",
    "\n",
    "###########################################\n",
    "# PHẦN 6: TRUY XUẤT THÔNG TIN ĐẦY ĐỦ TỪ VĂN BẢN PHÁP LUẬT\n",
    "###########################################\n",
    "\n",
    "def retrieve_law_info(query, doc_threshold=0.5, top_k=3):\n",
    "    \"\"\"\n",
    "    Nhận câu hỏi từ người dùng, chuyển đổi thành embedding và tính toán\n",
    "    cosine similarity với các đoạn văn bản pháp luật. Sau đó, lấy top_k đoạn\n",
    "    có điểm cao nhưng ghép chúng lại thành _một_ câu trả lời duy nhất.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_model.encode(query, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, paragraph_embeddings)[0]\n",
    "    top_scores, top_indices = torch.topk(cosine_scores, k=top_k)\n",
    "    \n",
    "    # Lọc các đoạn có điểm cao vượt ngưỡng\n",
    "    filtered_paragraphs = [paragraphs[idx] for score, idx in zip(top_scores, top_indices) if score.item() >= doc_threshold]\n",
    "    \n",
    "    if not filtered_paragraphs:\n",
    "        return \"⚠️ Xin lỗi, tôi không tìm thấy thông tin phù hợp trong các văn bản pháp luật.\"\n",
    "\n",
    "    # Ghép lại các đoạn đã lọc thành một câu trả lời duy nhất\n",
    "    aggregated_answer = \"\\n\\n\".join(filtered_paragraphs)\n",
    "    final_answer = \"📜 [Thông tin từ văn bản pháp luật]\\n\" + aggregated_answer\n",
    "\n",
    "    # Cập nhật vào Q&A database\n",
    "    update_qa_database(QA_DB_FILE, query, final_answer)\n",
    "    return final_answer\n",
    "\n",
    "###########################################\n",
    "# PHẦN 7: GIAO DIỆN CHATBOT\n",
    "###########################################\n",
    "\n",
    "def chatbot_loop():\n",
    "    print(\"\\n🤖 Chào mừng bạn đến với Chatbot tư vấn pháp luật!\")\n",
    "    print(\"Gõ 'exit' hoặc 'thoát' để kết thúc cuộc trò chuyện.\")\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(\"\\n📝 Bạn: \").strip()\n",
    "        if user_query.lower() in ['exit', 'thoát']:\n",
    "            print(\"👋 Chatbot: Cảm ơn bạn đã sử dụng dịch vụ!\")\n",
    "            break\n",
    "        # Lấy ra _một_ câu trả lời duy nhất, đầy đủ và ghép từ các đoạn liên quan\n",
    "        answer = retrieve_law_info(user_query)\n",
    "        print(f\"\\n💡 Chatbot:\\n{answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87698794-6128-4cf2-99d6-9f71673e74ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
