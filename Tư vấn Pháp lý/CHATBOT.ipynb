{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adebdd3d-8f4a-4c37-85fa-1872f6a4face",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2docx\n",
      "  Downloading pdf2docx-0.5.8-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: python-docx in c:\\users\\daoth\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\daoth\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\daoth\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\daoth\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\daoth\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Collecting PyMuPDF>=1.19.0 (from pdf2docx)\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: fonttools>=4.24.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pdf2docx) (4.51.0)\n",
      "Collecting opencv-python-headless>=4.5 (from pdf2docx)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting fire>=0.3.0 (from pdf2docx)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from python-docx) (4.11.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Collecting termcolor (from fire>=0.3.0->pdf2docx)\n",
      "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Downloading pdf2docx-0.5.8-py3-none-any.whl (132 kB)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "   ---------------------------------------- 0.0/39.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/39.4 MB 7.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.1/39.4 MB 5.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.9/39.4 MB 4.8 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 4.2/39.4 MB 5.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 5.2/39.4 MB 5.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.6/39.4 MB 5.4 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 8.1/39.4 MB 5.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 9.4/39.4 MB 5.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 11.0/39.4 MB 6.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 12.1/39.4 MB 5.9 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 13.4/39.4 MB 5.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 14.7/39.4 MB 5.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 16.0/39.4 MB 6.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 16.5/39.4 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 17.8/39.4 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 19.1/39.4 MB 5.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 20.2/39.4 MB 5.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 21.2/39.4 MB 5.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 22.3/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 23.6/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 24.9/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 26.0/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 27.0/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 28.0/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 29.4/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 30.4/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 31.7/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 33.0/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 34.1/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 35.4/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.7/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.7/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.4/39.4 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/16.6 MB 6.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.4/16.6 MB 5.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/16.6 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.0/16.6 MB 5.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 6.0/16.6 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.3/16.6 MB 5.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.7/16.6 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.7/16.6 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.7/16.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.1/16.6 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.4/16.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.4/16.6 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.0/16.6 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114262 sha256=7e68845a37ef53ab63fe7f338e3c04fa89969962f19a9e1c6db57f7897e3f51a\n",
      "  Stored in directory: c:\\users\\daoth\\appdata\\local\\pip\\cache\\wheels\\9e\\5b\\45\\29f72e55d87a29426b04b3cfdf20325c079eb97ab74f59017d\n",
      "Successfully built fire\n",
      "Installing collected packages: termcolor, PyMuPDF, opencv-python-headless, fire, pdf2docx\n",
      "Successfully installed PyMuPDF-1.25.5 fire-0.7.0 opencv-python-headless-4.11.0.86 pdf2docx-0.5.8 termcolor-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pdf2docx PyPDF2 python-docx sentence-transformers numpy scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27795228-f529-4fc3-a8ba-7e48f3cc04ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Use pytorch device_name: cpu\n",
      "[INFO] Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ t·∫£i n·ªôi dung t·ª´: data_crawl\\extracted_data.txt\n",
      "üìú S·ªë ƒëo·∫°n vƒÉn b·∫£n ph√°p lu·∫≠t tr√≠ch xu·∫•t: 9642\n",
      "\n",
      "üîé ƒêang t·∫°o embeddings cho vƒÉn b·∫£n ph√°p lu·∫≠t (qu√° tr√¨nh c√≥ th·ªÉ m·∫•t v√†i gi√¢y)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69829adc78cb40dc9ea3913bbee4791a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ho√†n th√†nh vi·ªác t·∫°o embeddings.\n",
      "\n",
      "ü§ñ Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi Chatbot t∆∞ v·∫•n ph√°p lu·∫≠t!\n",
      "G√µ 'exit' ho·∫∑c 'tho√°t' ƒë·ªÉ k·∫øt th√∫c cu·ªôc tr√≤ chuy·ªán.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù B·∫°n:   T·∫°i sao ƒë·∫•t ƒëai ·ªü Vi·ªát Nam thu·ªôc s·ªü h·ªØu to√†n d√¢n do Nh√† n∆∞·ªõc qu·∫£n l√Ω?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c0f92a8e534b6b922b25b04aab4871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Chatbot:\n",
      "üìú [Th√¥ng tin t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t]\n",
      "1. Nh√† n∆∞·ªõc C·ªông h√≤a x√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam l√† ƒë·∫°i di·ªán, th·ª±c hi·ªán quy·ªÅn c·ªßa ch·ªß s·ªü h·ªØu ƒë·ªëi v·ªõi t√†i s·∫£n thu·ªôc s·ªü h·ªØu to√†n d√¢n.\n",
      "\n",
      "1. Nh√† n∆∞·ªõc C·ªông h√≤a x√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam l√† ƒë·∫°i di·ªán, th·ª±c hi·ªán quy·ªÅn c·ªßa ch·ªß s·ªü h·ªØu ƒë·ªëi v·ªõi t√†i s·∫£n thu·ªôc s·ªü h·ªØu to√†n d√¢n.\n",
      "\n",
      "1. Nh√† n∆∞·ªõc C·ªông h√≤a x√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam, c∆° quan nh√† n∆∞·ªõc ·ªü Trung ∆∞∆°ng, ·ªü ƒë·ªãa ph∆∞∆°ng ch·ªãu tr√°ch nhi·ªám v·ªÅ nghƒ©a v·ª• d√¢n s·ª± c·ªßa m√¨nh b·∫±ng t√†i s·∫£n m√† m√¨nh l√† ƒë·∫°i di·ªán ch·ªß s·ªü h·ªØu v√† th·ªëng nh·∫•t qu·∫£n l√Ω, tr·ª´ tr∆∞·ªùng h·ª£p t√†i s·∫£n ƒë√£ ƒë∆∞·ª£c chuy·ªÉn giao cho ph√°p nh√¢n theo quy ƒë·ªãnh t·∫°i kho·∫£n 2 ƒêi·ªÅu n√†y.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù B·∫°n:  hi Nh√† n∆∞·ªõc thu h·ªìi ƒë·∫•t, ng∆∞·ªùi d√¢n c√≥ quy·ªÅn l·ª£i g√¨? Ch√≠nh s√°ch b·ªìi th∆∞·ªùng ƒë∆∞·ª£c √°p d·ª•ng nh∆∞ th·∫ø n√†o?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86cad5823134c9badde740478a5dbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Chatbot:\n",
      "üìú [Th√¥ng tin t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t]\n",
      "Nh√† n∆∞·ªõc c√≥ tr√°ch nhi·ªám b·ªìi th∆∞·ªùng thi·ªát h·∫°i v√† ph·ª•c h·ªìi danh d·ª±, quy·ªÅn l·ª£i cho ng∆∞·ªùi b·ªãgi·ªØ trong tr∆∞·ªùng h·ª£p kh·∫©n c·∫•p, ng∆∞·ªùi b·ªã b·∫Øt, b·ªãt·∫°m gi·ªØ, t·∫°m giam, kh·ªüi t·ªë, ƒëi·ªÅu tra, truy t·ªë, x√©t x·ª≠, thi h√†nh √°n oan, tr√°i ph√°p lu·∫≠t do c∆° quan, ng∆∞·ªùi c√≥ th·∫©m quy·ªÅn ti√™ÃÅn haÃÄnh t·ªë t·ª•ng g√¢y ra.\n",
      "\n",
      "Nh√† n∆∞·ªõc c√≥ tr√°ch nhi·ªám b·ªìi th∆∞·ªùng thi·ªát h·∫°i v√† ph·ª•c h·ªìi danh d·ª±, quy·ªÅn l·ª£i cho ng∆∞·ªùi b·ªãgi·ªØ trong tr∆∞·ªùng h·ª£p kh·∫©n c·∫•p, ng∆∞·ªùi b·ªã b·∫Øt, b·ªãt·∫°m gi·ªØ, t·∫°m giam, kh·ªüi t·ªë, ƒëi·ªÅu tra, truy t·ªë, x√©t x·ª≠, thi h√†nh √°n oan, tr√°i ph√°p lu·∫≠t do c∆° quan, ng∆∞·ªùi c√≥ th·∫©m quy·ªÅn ti√™ÃÅn haÃÄnh t·ªë t·ª•ng g√¢y ra.\n",
      "\n",
      "d) ƒê∆∞·ª£c b·ªìi th∆∞·ªùng thi·ªát h·∫°i, kh√¥i ph·ª•c danh d·ª±, b·∫£o ƒë·∫£m c√°c quy·ªÅn v√† l·ª£i √≠ch h·ª£p ph√°p trong th·ªùi gian b·∫£o v·ªá.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù B·∫°n:   Nh√† n∆∞·ªõc quy ƒë·ªãnh gi√° ƒë·∫•t nh∆∞ th·∫ø n√†o? C√≥ nh·ªØng b·∫•t c·∫≠p g√¨ trong vi·ªác ƒë·ªãnh gi√° ƒë·∫•t?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f43d69a53f3439a86e5553989aeee04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Chatbot:\n",
      "üìú [Th√¥ng tin t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t]\n",
      "ƒêi·ªÅu 325.Th·∫ø ch·∫•p quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t m√† kh√¥ng th·∫ø ch·∫•p t√†i s·∫£n g·∫Øn li·ªÅn v·ªõi ƒë·∫•t\n",
      "\n",
      "ƒêi·ªÅu 325.Th·∫ø ch·∫•p quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t m√† kh√¥ng th·∫ø ch·∫•p t√†i s·∫£n g·∫Øn li·ªÅn v·ªõi ƒë·∫•t\n",
      "\n",
      "ƒêi·ªÅu 326.Th·∫ø ch·∫•p t√†i s·∫£n g·∫Øn li·ªÅn v·ªõi ƒë·∫•t m√† kh√¥ng th·∫ø ch·∫•p quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù B·∫°n:  tho√°t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã Chatbot: C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng d·ªãch v·ª•!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "from pdf2docx import Converter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "###########################################\n",
    "# PH·∫¶N 1: CHUY·ªÇN ƒê·ªîI V√Ä TR√çCH XU·∫§T VƒÇN B·∫¢N T·ª™ TH∆Ø M·ª§C data_cawl\n",
    "###########################################\n",
    "\n",
    "DATA_FOLDER = \"data_crawl\"\n",
    "EXTRACTED_TEXT_FILE = os.path.join(DATA_FOLDER, \"extracted_data.txt\")\n",
    "\n",
    "def convert_pdf_to_docx(pdf_path, docx_path):\n",
    "    \"\"\"Chuy·ªÉn ƒë·ªïi file PDF sang DOCX ƒë·ªÉ tr√≠ch xu·∫•t n·ªôi dung.\"\"\"\n",
    "    try:\n",
    "        cv = Converter(pdf_path)\n",
    "        cv.convert(docx_path, start=0, end=None)\n",
    "        cv.close()\n",
    "        print(f\"‚úÖ ƒê√£ chuy·ªÉn ƒë·ªïi PDF sang DOCX: {docx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói khi chuy·ªÉn ƒë·ªïi PDF {pdf_path}: {e}\")\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    \"\"\"Tr√≠ch xu·∫•t n·ªôi dung t·ª´ file DOCX.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        for para in doc.paragraphs:\n",
    "            if para.text.strip():\n",
    "                text += para.text.strip() + \"\\n\"\n",
    "        print(f\"‚úÖ ƒê√£ tr√≠ch xu·∫•t n·ªôi dung t·ª´: {docx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói khi ƒë·ªçc DOCX {docx_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def process_all_documents(folder_path):\n",
    "    \"\"\"\n",
    "    Duy·ªát qua th∆∞ m·ª•c data_cawl, chuy·ªÉn PDF sang DOCX (n·∫øu c·∫ßn)\n",
    "    v√† tr√≠ch xu·∫•t n·ªôi dung t·ª´ c√°c file DOCX.\n",
    "    \"\"\"\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # N·∫øu l√† PDF th√¨ chuy·ªÉn sang DOCX tr∆∞·ªõc\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            docx_path = file_path[:-4] + \".docx\"  # thay .pdf th√†nh .docx\n",
    "            convert_pdf_to_docx(file_path, docx_path)\n",
    "            all_texts.append(extract_text_from_docx(docx_path))\n",
    "        elif filename.lower().endswith(\".docx\"):\n",
    "            all_texts.append(extract_text_from_docx(file_path))\n",
    "    return \"\\n\".join(all_texts)\n",
    "\n",
    "def save_extracted_text(text, filename=EXTRACTED_TEXT_FILE):\n",
    "    \"\"\"L∆∞u vƒÉn b·∫£n ƒë√£ tr√≠ch xu·∫•t v√†o file TXT.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"‚úÖ ƒê√£ l∆∞u vƒÉn b·∫£n tr√≠ch xu·∫•t v√†o: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói khi l∆∞u vƒÉn b·∫£n: {e}\")\n",
    "\n",
    "def load_extracted_text(filename=EXTRACTED_TEXT_FILE):\n",
    "    \"\"\"T·∫£i vƒÉn b·∫£n ƒë√£ tr√≠ch xu·∫•t t·ª´ file TXT.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        print(f\"‚úÖ ƒê√£ t·∫£i n·ªôi dung t·ª´: {filename}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói khi t·∫£i vƒÉn b·∫£n t·ª´ file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# N·∫øu file vƒÉn b·∫£n ƒë√£ c√≥, t·∫£i tr·ª±c ti·∫øp t·ª´ file; n·∫øu kh√¥ng, x·ª≠ l√Ω g·ªëc v√† l∆∞u l·∫°i.\n",
    "if os.path.exists(EXTRACTED_TEXT_FILE):\n",
    "    combined_text = load_extracted_text()\n",
    "else:\n",
    "    combined_text = process_all_documents(DATA_FOLDER)\n",
    "    save_extracted_text(combined_text)\n",
    "\n",
    "###########################################\n",
    "# PH·∫¶N 2: TI·ªÄN X·ª¨ L√ù VƒÇN B·∫¢N ‚Äì CHIA TH√ÄNH C√ÅC ƒêO·∫†N\n",
    "###########################################\n",
    "\n",
    "def split_into_paragraphs(text, min_length=50):\n",
    "    \"\"\"Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n c√≥ √Ω nghƒ©a, b·ªè qua c√°c ƒëo·∫°n qu√° ng·∫Øn.\"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\") if len(p.strip()) >= min_length]\n",
    "    return paragraphs\n",
    "\n",
    "paragraphs = split_into_paragraphs(combined_text)\n",
    "print(f\"üìú S·ªë ƒëo·∫°n vƒÉn b·∫£n ph√°p lu·∫≠t tr√≠ch xu·∫•t: {len(paragraphs)}\")\n",
    "\n",
    "###########################################\n",
    "# PH·∫¶N 3: T·∫†O EMBEDDINGS CHO C√ÅC ƒêO·∫†N VƒÇN B·∫¢N\n",
    "###########################################\n",
    "\n",
    "embed_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "print(\"\\nüîé ƒêang t·∫°o embeddings cho vƒÉn b·∫£n ph√°p lu·∫≠t (qu√° tr√¨nh c√≥ th·ªÉ m·∫•t v√†i gi√¢y)...\")\n",
    "paragraph_embeddings = embed_model.encode(paragraphs, convert_to_tensor=True)\n",
    "print(\"‚úÖ Ho√†n th√†nh vi·ªác t·∫°o embeddings.\")\n",
    "\n",
    "###########################################\n",
    "# PH·∫¶N 4: TR√çCH XU·∫§T TH√îNG TIN 'ƒêI·ªÄU'\n",
    "###########################################\n",
    "\n",
    "def extract_article_info(text):\n",
    "    \"\"\"\n",
    "    S·ª≠ d·ª•ng regex ƒë·ªÉ t√¨m ki·∫øm m·∫´u nh∆∞ \"ƒêi·ªÅu 15\" ho·∫∑c \"ƒêi·ªÅu15.1\" trong ƒëo·∫°n vƒÉn.\n",
    "    Tr·∫£ v·ªÅ th√¥ng tin 'ƒêi·ªÅu' n·∫øu t√¨m th·∫•y, ng∆∞·ª£c l·∫°i tr·∫£ v·ªÅ None.\n",
    "    \"\"\"\n",
    "    pattern = r'ƒêi·ªÅu\\s*\\d+([.,]\\d+)?'\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "###########################################\n",
    "# PH·∫¶N 5: C·∫¨P NH·∫¨T C√ÇU H·ªéI ‚Äì C√ÇU TR·∫¢ L·ªúI V√ÄO FILE CSV (QA DATABASE)\n",
    "###########################################\n",
    "\n",
    "QA_DB_FILE = \"qa_database.csv\"\n",
    "\n",
    "def load_qa_database(filename):\n",
    "    \"\"\"ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV n·∫øu t·ªìn t·∫°i.\"\"\"\n",
    "    data = []\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, mode='r', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                data.append(row)\n",
    "    return data\n",
    "\n",
    "def save_qa_database(filename, data):\n",
    "    \"\"\"Ghi danh s√°ch c√°c c√¢u h·ªèi ‚Äì c√¢u tr·∫£ l·ªùi v√†o file CSV.\"\"\"\n",
    "    fieldnames = ['question', 'answer', 'created_at', 'updated_at']\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def update_qa_database(filename, new_question, new_answer):\n",
    "    \"\"\"\n",
    "    N·∫øu c√¢u h·ªèi ƒë√£ c√≥ (so s√°nh kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng), c·∫≠p nh·∫≠t c√¢u tr·∫£ l·ªùi v√† th·ªùi gian;\n",
    "    N·∫øu ch∆∞a c√≥ th√¨ th√™m m·ªõi.\n",
    "    \"\"\"\n",
    "    data = load_qa_database(filename)\n",
    "    found = False\n",
    "    for entry in data:\n",
    "        if entry['question'].strip().lower() == new_question.strip().lower():\n",
    "            entry['answer'] = new_answer\n",
    "            entry['updated_at'] = datetime.now().isoformat()\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        now_str = datetime.now().isoformat()\n",
    "        data.append({\n",
    "            'question': new_question,\n",
    "            'answer': new_answer,\n",
    "            'created_at': now_str,\n",
    "            'updated_at': now_str\n",
    "        })\n",
    "    save_qa_database(filename, data)\n",
    "    return data\n",
    "\n",
    "###########################################\n",
    "# PH·∫¶N 6: TRUY XU·∫§T TH√îNG TIN ƒê·∫¶Y ƒê·ª¶ T·ª™ VƒÇN B·∫¢N PH√ÅP LU·∫¨T\n",
    "###########################################\n",
    "\n",
    "def retrieve_law_info(query, doc_threshold=0.5, top_k=3):\n",
    "    \"\"\"\n",
    "    Nh·∫≠n c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng, chuy·ªÉn ƒë·ªïi th√†nh embedding v√† t√≠nh to√°n\n",
    "    cosine similarity v·ªõi c√°c ƒëo·∫°n vƒÉn b·∫£n ph√°p lu·∫≠t. Sau ƒë√≥, l·∫•y top_k ƒëo·∫°n\n",
    "    c√≥ ƒëi·ªÉm cao nh∆∞ng gh√©p ch√∫ng l·∫°i th√†nh _m·ªôt_ c√¢u tr·∫£ l·ªùi duy nh·∫•t.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_model.encode(query, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, paragraph_embeddings)[0]\n",
    "    top_scores, top_indices = torch.topk(cosine_scores, k=top_k)\n",
    "    \n",
    "    # L·ªçc c√°c ƒëo·∫°n c√≥ ƒëi·ªÉm cao v∆∞·ª£t ng∆∞·ª°ng\n",
    "    filtered_paragraphs = [paragraphs[idx] for score, idx in zip(top_scores, top_indices) if score.item() >= doc_threshold]\n",
    "    \n",
    "    if not filtered_paragraphs:\n",
    "        return \"‚ö†Ô∏è Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong c√°c vƒÉn b·∫£n ph√°p lu·∫≠t.\"\n",
    "\n",
    "    # Gh√©p l·∫°i c√°c ƒëo·∫°n ƒë√£ l·ªçc th√†nh m·ªôt c√¢u tr·∫£ l·ªùi duy nh·∫•t\n",
    "    aggregated_answer = \"\\n\\n\".join(filtered_paragraphs)\n",
    "    final_answer = \"üìú [Th√¥ng tin t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t]\\n\" + aggregated_answer\n",
    "\n",
    "    # C·∫≠p nh·∫≠t v√†o Q&A database\n",
    "    update_qa_database(QA_DB_FILE, query, final_answer)\n",
    "    return final_answer\n",
    "\n",
    "###########################################\n",
    "# PH·∫¶N 7: GIAO DI·ªÜN CHATBOT\n",
    "###########################################\n",
    "\n",
    "def chatbot_loop():\n",
    "    print(\"\\nü§ñ Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi Chatbot t∆∞ v·∫•n ph√°p lu·∫≠t!\")\n",
    "    print(\"G√µ 'exit' ho·∫∑c 'tho√°t' ƒë·ªÉ k·∫øt th√∫c cu·ªôc tr√≤ chuy·ªán.\")\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(\"\\nüìù B·∫°n: \").strip()\n",
    "        if user_query.lower() in ['exit', 'tho√°t']:\n",
    "            print(\"üëã Chatbot: C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng d·ªãch v·ª•!\")\n",
    "            break\n",
    "        # L·∫•y ra _m·ªôt_ c√¢u tr·∫£ l·ªùi duy nh·∫•t, ƒë·∫ßy ƒë·ªß v√† gh√©p t·ª´ c√°c ƒëo·∫°n li√™n quan\n",
    "        answer = retrieve_law_info(user_query)\n",
    "        print(f\"\\nüí° Chatbot:\\n{answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87698794-6128-4cf2-99d6-9f71673e74ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
