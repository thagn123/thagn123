import os
import re
import csv
from datetime import datetime
import PyPDF2
from docx import Document
from pdf2docx import Converter
from sentence_transformers import SentenceTransformer, util
import torch
import numpy as np

###########################################
# PHáº¦N 1: CHUYá»‚N Äá»”I VÃ€ TRÃCH XUáº¤T VÄ‚N Báº¢N Tá»ª THÆ¯ Má»¤C data_cawl
###########################################

DATA_FOLDER = "data_cawl"
EXTRACTED_TEXT_FILE = os.path.join(DATA_FOLDER, "extracted_data.txt")


def convert_pdf_to_docx(pdf_path, docx_path):
    """Chuyá»ƒn Ä‘á»•i file PDF sang DOCX Ä‘á»ƒ trÃ­ch xuáº¥t ná»™i dung."""
    try:
        cv = Converter(pdf_path)
        cv.convert(docx_path, start=0, end=None)
        cv.close()
        print(f"âœ… ÄÃ£ chuyá»ƒn Ä‘á»•i PDF sang DOCX: {docx_path}")
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi chuyá»ƒn Ä‘á»•i PDF {pdf_path}: {e}")


def extract_text_from_docx(docx_path):
    """TrÃ­ch xuáº¥t ná»™i dung tá»« file DOCX."""
    text = ""
    try:
        doc = Document(docx_path)
        for para in doc.paragraphs:
            if para.text.strip():
                text += para.text.strip() + "\n"
        print(f"âœ… ÄÃ£ trÃ­ch xuáº¥t ná»™i dung tá»«: {docx_path}")
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi Ä‘á»c DOCX {docx_path}: {e}")
    return text


def process_all_documents(folder_path):
    """
    Duyá»‡t qua thÆ° má»¥c data_cawl, chuyá»ƒn PDF sang DOCX (náº¿u cáº§n)
    vÃ  trÃ­ch xuáº¥t ná»™i dung tá»« cÃ¡c file DOCX.
    """
    all_texts = []
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        # Náº¿u lÃ  PDF thÃ¬ chuyá»ƒn sang DOCX trÆ°á»›c
        if filename.lower().endswith(".pdf"):
            docx_path = file_path[:-4] + ".docx"  # thay .pdf thÃ nh .docx
            convert_pdf_to_docx(file_path, docx_path)
            all_texts.append(extract_text_from_docx(docx_path))
        elif filename.lower().endswith(".docx"):
            all_texts.append(extract_text_from_docx(file_path))
    return "\n".join(all_texts)


def save_extracted_text(text, filename=EXTRACTED_TEXT_FILE):
    """LÆ°u vÄƒn báº£n Ä‘Ã£ trÃ­ch xuáº¥t vÃ o file TXT."""
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(text)
        print(f"âœ… ÄÃ£ lÆ°u vÄƒn báº£n trÃ­ch xuáº¥t vÃ o: {filename}")
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi lÆ°u vÄƒn báº£n: {e}")


def load_extracted_text(filename=EXTRACTED_TEXT_FILE):
    """Táº£i vÄƒn báº£n Ä‘Ã£ trÃ­ch xuáº¥t tá»« file TXT."""
    try:
        with open(filename, "r", encoding="utf-8") as f:
            text = f.read()
        print(f"âœ… ÄÃ£ táº£i ná»™i dung tá»«: {filename}")
        return text
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi táº£i vÄƒn báº£n tá»« file: {e}")
        return ""


# Náº¿u file vÄƒn báº£n Ä‘Ã£ cÃ³, táº£i trá»±c tiáº¿p tá»« file; náº¿u khÃ´ng, xá»­ lÃ½ gá»‘c vÃ  lÆ°u láº¡i.
if os.path.exists(EXTRACTED_TEXT_FILE):
    combined_text = load_extracted_text()
else:
    combined_text = process_all_documents(DATA_FOLDER)
    save_extracted_text(combined_text)


###########################################
# PHáº¦N 2: TIá»€N Xá»¬ LÃ VÄ‚N Báº¢N â€“ CHIA THÃ€NH CÃC ÄOáº N
###########################################

def split_into_paragraphs(text, min_length=50):
    """Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n cÃ³ Ã½ nghÄ©a, bá» qua cÃ¡c Ä‘oáº¡n quÃ¡ ngáº¯n."""
    paragraphs = [p.strip() for p in text.split("\n") if len(p.strip()) >= min_length]
    return paragraphs


paragraphs = split_into_paragraphs(combined_text)
print(f"ğŸ“œ Sá»‘ Ä‘oáº¡n vÄƒn báº£n phÃ¡p luáº­t trÃ­ch xuáº¥t: {len(paragraphs)}")

###########################################
# PHáº¦N 3: Táº O EMBEDDINGS CHO CÃC ÄOáº N VÄ‚N Báº¢N
###########################################

embed_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
print("\n Äang táº¡o embeddings cho vÄƒn báº£n phÃ¡p luáº­t (quÃ¡ trÃ¬nh cÃ³ thá»ƒ máº¥t vÃ i giÃ¢y)...")
paragraph_embeddings = embed_model.encode(paragraphs, convert_to_tensor=True)
print(" HoÃ n thÃ nh viá»‡c táº¡o embeddings.")


###########################################
# PHáº¦N 4: TRÃCH XUáº¤T THÃ”NG TIN 'ÄIá»€U'
###########################################

def extract_article_info(text):
    """
    Sá»­ dá»¥ng regex Ä‘á»ƒ tÃ¬m kiáº¿m máº«u nhÆ° "Äiá»u 15" hoáº·c "Äiá»u15.1" trong Ä‘oáº¡n vÄƒn.
    Tráº£ vá» thÃ´ng tin 'Äiá»u' náº¿u tÃ¬m tháº¥y, ngÆ°á»£c láº¡i tráº£ vá» None.
    """
    pattern = r'Äiá»u\s*\d+([.,]\d+)?'
    match = re.search(pattern, text, re.IGNORECASE)
    if match:
        return match.group(0)
    return None


###########################################
# PHáº¦N 5: Cáº¬P NHáº¬T CÃ‚U Há»I â€“ CÃ‚U TRáº¢ Lá»œI VÃ€O FILE CSV (QA DATABASE)
###########################################

QA_DB_FILE = "qa_database.csv"


def load_qa_database(filename):
    """Äá»c dá»¯ liá»‡u tá»« file CSV náº¿u tá»“n táº¡i."""
    data = []
    if os.path.exists(filename):
        with open(filename, mode='r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                data.append(row)
    return data


def save_qa_database(filename, data):
    """Ghi danh sÃ¡ch cÃ¡c cÃ¢u há»i â€“ cÃ¢u tráº£ lá»i vÃ o file CSV."""
    fieldnames = ['question', 'answer', 'created_at', 'updated_at']
    with open(filename, mode='w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for row in data:
            writer.writerow(row)


def update_qa_database(filename, new_question, new_answer):
    """
    Náº¿u cÃ¢u há»i Ä‘Ã£ cÃ³ (so sÃ¡nh khÃ´ng phÃ¢n biá»‡t hoa thÆ°á»ng), cáº­p nháº­t cÃ¢u tráº£ lá»i vÃ  thá»i gian;
    Náº¿u chÆ°a cÃ³ thÃ¬ thÃªm má»›i.
    """
    data = load_qa_database(filename)
    found = False
    for entry in data:
        if entry['question'].strip().lower() == new_question.strip().lower():
            entry['answer'] = new_answer
            entry['updated_at'] = datetime.now().isoformat()
            found = True
            break
    if not found:
        now_str = datetime.now().isoformat()
        data.append({
            'question': new_question,
            'answer': new_answer,
            'created_at': now_str,
            'updated_at': now_str
        })
    save_qa_database(filename, data)
    return data


###########################################
# PHáº¦N 6: TRUY XUáº¤T THÃ”NG TIN Äáº¦Y Äá»¦ Tá»ª VÄ‚N Báº¢N PHÃP LUáº¬T
###########################################

def retrieve_law_info(query, doc_threshold=0.5, top_k=3):
    """
    Nháº­n cÃ¢u há»i tá»« ngÆ°á»i dÃ¹ng, chuyá»ƒn Ä‘á»•i thÃ nh embedding vÃ  tÃ­nh toÃ¡n
    cosine similarity vá»›i cÃ¡c Ä‘oáº¡n vÄƒn báº£n phÃ¡p luáº­t. Sau Ä‘Ã³, láº¥y top_k Ä‘oáº¡n
    cÃ³ Ä‘iá»ƒm cao nhÆ°ng ghÃ©p chÃºng láº¡i thÃ nh _má»™t_ cÃ¢u tráº£ lá»i duy nháº¥t.
    """
    query_embedding = embed_model.encode(query, convert_to_tensor=True)
    cosine_scores = util.cos_sim(query_embedding, paragraph_embeddings)[0]
    top_scores, top_indices = torch.topk(cosine_scores, k=top_k)

    # Lá»c cÃ¡c Ä‘oáº¡n cÃ³ Ä‘iá»ƒm cao vÆ°á»£t ngÆ°á»¡ng
    filtered_paragraphs = [paragraphs[idx] for score, idx in zip(top_scores, top_indices) if
                           score.item() >= doc_threshold]

    if not filtered_paragraphs:
        return "âš ï¸ Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p trong cÃ¡c vÄƒn báº£n phÃ¡p luáº­t."

    # GhÃ©p láº¡i cÃ¡c Ä‘oáº¡n Ä‘Ã£ lá»c thÃ nh má»™t cÃ¢u tráº£ lá»i duy nháº¥t
    aggregated_answer = "\n\n".join(filtered_paragraphs)
    final_answer = "ğŸ“œ [ThÃ´ng tin tá»« vÄƒn báº£n phÃ¡p luáº­t]\n" + aggregated_answer

    # Cáº­p nháº­t vÃ o Q&A database
    update_qa_database(QA_DB_FILE, query, final_answer)
    return final_answer


###########################################
# PHáº¦N 7: GIAO DIá»†N CHATBOT
###########################################

def chatbot_loop():
    print("\nğŸ¤– ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i Chatbot tÆ° váº¥n phÃ¡p luáº­t!")
    print("GÃµ 'exit' hoáº·c 'thoÃ¡t' Ä‘á»ƒ káº¿t thÃºc cuá»™c trÃ² chuyá»‡n.")

    while True:
        user_query = input("\nğŸ“ Báº¡n: ").strip()
        if user_query.lower() in ['exit', 'thoÃ¡t']:
            print("ğŸ‘‹ Chatbot: Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng dá»‹ch vá»¥!")
            break
        # Láº¥y ra _má»™t_ cÃ¢u tráº£ lá»i duy nháº¥t, Ä‘áº§y Ä‘á»§ vÃ  ghÃ©p tá»« cÃ¡c Ä‘oáº¡n liÃªn quan
        answer = retrieve_law_info(user_query)
        print(f"\nğŸ’¡ Chatbot:\n{answer}")


if __name__ == "__main__":
    chatbot_loop()