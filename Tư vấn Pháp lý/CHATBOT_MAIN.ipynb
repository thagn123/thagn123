{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adebdd3d-8f4a-4c37-85fa-1872f6a4face",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2docx\n",
      "  Downloading pdf2docx-0.5.8-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: python-docx in c:\\users\\daoth\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\daoth\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\daoth\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\daoth\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\daoth\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Collecting PyMuPDF>=1.19.0 (from pdf2docx)\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: fonttools>=4.24.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pdf2docx) (4.51.0)\n",
      "Collecting opencv-python-headless>=4.5 (from pdf2docx)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting fire>=0.3.0 (from pdf2docx)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from python-docx) (4.11.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Collecting termcolor (from fire>=0.3.0->pdf2docx)\n",
      "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Downloading pdf2docx-0.5.8-py3-none-any.whl (132 kB)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "   ---------------------------------------- 0.0/39.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/39.4 MB 7.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.1/39.4 MB 5.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.9/39.4 MB 4.8 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 4.2/39.4 MB 5.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 5.2/39.4 MB 5.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.6/39.4 MB 5.4 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 8.1/39.4 MB 5.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 9.4/39.4 MB 5.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 11.0/39.4 MB 6.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 12.1/39.4 MB 5.9 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 13.4/39.4 MB 5.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 14.7/39.4 MB 5.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 16.0/39.4 MB 6.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 16.5/39.4 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 17.8/39.4 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 19.1/39.4 MB 5.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 20.2/39.4 MB 5.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 21.2/39.4 MB 5.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 22.3/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 23.6/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 24.9/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 26.0/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 27.0/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 28.0/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 29.4/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 30.4/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 31.7/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 33.0/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 34.1/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 35.4/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.7/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.7/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.4/39.4 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/16.6 MB 6.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.4/16.6 MB 5.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/16.6 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.0/16.6 MB 5.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 6.0/16.6 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.3/16.6 MB 5.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.7/16.6 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.7/16.6 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.7/16.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.1/16.6 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.4/16.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.4/16.6 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.0/16.6 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114262 sha256=7e68845a37ef53ab63fe7f338e3c04fa89969962f19a9e1c6db57f7897e3f51a\n",
      "  Stored in directory: c:\\users\\daoth\\appdata\\local\\pip\\cache\\wheels\\9e\\5b\\45\\29f72e55d87a29426b04b3cfdf20325c079eb97ab74f59017d\n",
      "Successfully built fire\n",
      "Installing collected packages: termcolor, PyMuPDF, opencv-python-headless, fire, pdf2docx\n",
      "Successfully installed PyMuPDF-1.25.5 fire-0.7.0 opencv-python-headless-4.11.0.86 pdf2docx-0.5.8 termcolor-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pdf2docx PyPDF2 python-docx sentence-transformers numpy scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f5537-affb-446c-b9dd-296fb89c9b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2fa90-db94-48a1-9ef9-4061a3a22d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "from pdf2docx import Converter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f0c9f-4fa5-4b6c-94ca-bbbe8a438290",
   "metadata": {},
   "source": [
    "# PHáº¦N 1: CHUYá»‚N Äá»”I VÃ€ TRÃCH XUáº¤T VÄ‚N Báº¢N Tá»ª THÆ¯ Má»¤C data_cawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c00a7-dba2-4e09-9989-6837b9f30639",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data_crawl\"\n",
    "EXTRACTED_TEXT_FILE = os.path.join(DATA_FOLDER, \"extracted_data.txt\")\n",
    "\n",
    "def convert_pdf_to_docx(pdf_path, docx_path):\n",
    "    \"\"\"Chuyá»ƒn Ä‘á»•i file PDF sang DOCX Ä‘á»ƒ trÃ­ch xuáº¥t ná»™i dung.\"\"\"\n",
    "    try:\n",
    "        cv = Converter(pdf_path)\n",
    "        cv.convert(docx_path, start=0, end=None)\n",
    "        cv.close()\n",
    "        print(f\"âœ… ÄÃ£ chuyá»ƒn Ä‘á»•i PDF sang DOCX: {docx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Lá»—i khi chuyá»ƒn Ä‘á»•i PDF {pdf_path}: {e}\")\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    \"\"\"TrÃ­ch xuáº¥t ná»™i dung tá»« file DOCX.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        for para in doc.paragraphs:\n",
    "            if para.text.strip():\n",
    "                text += para.text.strip() + \"\\n\"\n",
    "        print(f\"âœ… ÄÃ£ trÃ­ch xuáº¥t ná»™i dung tá»«: {docx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Lá»—i khi Ä‘á»c DOCX {docx_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def process_all_documents(folder_path):\n",
    "    \"\"\"\n",
    "    Duyá»‡t qua thÆ° má»¥c data_cawl, chuyá»ƒn PDF sang DOCX (náº¿u cáº§n)\n",
    "    vÃ  trÃ­ch xuáº¥t ná»™i dung tá»« cÃ¡c file DOCX.\n",
    "    \"\"\"\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Náº¿u lÃ  PDF thÃ¬ chuyá»ƒn sang DOCX trÆ°á»›c\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            docx_path = file_path[:-4] + \".docx\"  # thay .pdf thÃ nh .docx\n",
    "            convert_pdf_to_docx(file_path, docx_path)\n",
    "            all_texts.append(extract_text_from_docx(docx_path))\n",
    "        elif filename.lower().endswith(\".docx\"):\n",
    "            all_texts.append(extract_text_from_docx(file_path))\n",
    "    return \"\\n\".join(all_texts)\n",
    "\n",
    "def save_extracted_text(text, filename=EXTRACTED_TEXT_FILE):\n",
    "    \"\"\"LÆ°u vÄƒn báº£n Ä‘Ã£ trÃ­ch xuáº¥t vÃ o file TXT.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"âœ… ÄÃ£ lÆ°u vÄƒn báº£n trÃ­ch xuáº¥t vÃ o: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Lá»—i khi lÆ°u vÄƒn báº£n: {e}\")\n",
    "\n",
    "def load_extracted_text(filename=EXTRACTED_TEXT_FILE):\n",
    "    \"\"\"Táº£i vÄƒn báº£n Ä‘Ã£ trÃ­ch xuáº¥t tá»« file TXT.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        print(f\"âœ… ÄÃ£ táº£i ná»™i dung tá»«: {filename}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Lá»—i khi táº£i vÄƒn báº£n tá»« file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Náº¿u file vÄƒn báº£n Ä‘Ã£ cÃ³, táº£i trá»±c tiáº¿p tá»« file; náº¿u khÃ´ng, xá»­ lÃ½ gá»‘c vÃ  lÆ°u láº¡i.\n",
    "if os.path.exists(EXTRACTED_TEXT_FILE):\n",
    "    combined_text = load_extracted_text()\n",
    "else:\n",
    "    combined_text = process_all_documents(DATA_FOLDER)\n",
    "    save_extracted_text(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdb443-f02f-4a0f-aef4-937a47679042",
   "metadata": {},
   "source": [
    "# PHáº¦N 2: TIá»€N Xá»¬ LÃ VÄ‚N Báº¢N â€“ CHIA THÃ€NH CÃC ÄOáº N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87698794-6128-4cf2-99d6-9f71673e74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_paragraphs(text, min_length=50):\n",
    "    \"\"\"Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n cÃ³ Ã½ nghÄ©a, bá» qua cÃ¡c Ä‘oáº¡n quÃ¡ ngáº¯n.\"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\") if len(p.strip()) >= min_length]\n",
    "    return paragraphs\n",
    "\n",
    "paragraphs = split_into_paragraphs(combined_text)\n",
    "print(f\"ğŸ“œ Sá»‘ Ä‘oáº¡n vÄƒn báº£n phÃ¡p luáº­t trÃ­ch xuáº¥t: {len(paragraphs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d4583-9318-4cec-a1de-5317b66d9aa3",
   "metadata": {},
   "source": [
    "# PHáº¦N 3: Táº O EMBEDDINGS CHO CÃC ÄOáº N VÄ‚N Báº¢N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71d509-59fd-4fb7-bb33-a2fe1d97ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "print(\"\\nğŸ” Äang táº¡o embeddings cho vÄƒn báº£n phÃ¡p luáº­t (quÃ¡ trÃ¬nh cÃ³ thá»ƒ máº¥t vÃ i giÃ¢y)...\")\n",
    "paragraph_embeddings = embed_model.encode(paragraphs, convert_to_tensor=True)\n",
    "print(\"âœ… HoÃ n thÃ nh viá»‡c táº¡o embeddings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c472dbcf-95ef-48fb-81c0-9ac356264d43",
   "metadata": {},
   "source": [
    "# PHáº¦N 4: TRÃCH XUáº¤T THÃ”NG TIN 'ÄIá»€U'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b0b98-50ff-4ca9-9992-56f513744b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_info(text):\n",
    "    \"\"\"\n",
    "    Sá»­ dá»¥ng regex Ä‘á»ƒ tÃ¬m kiáº¿m máº«u nhÆ° \"Äiá»u 15\" hoáº·c \"Äiá»u15.1\" trong Ä‘oáº¡n vÄƒn.\n",
    "    Tráº£ vá» thÃ´ng tin 'Äiá»u' náº¿u tÃ¬m tháº¥y, ngÆ°á»£c láº¡i tráº£ vá» None.\n",
    "    \"\"\"\n",
    "    pattern = r'Äiá»u\\s*\\d+([.,]\\d+)?'\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64178d2-7429-4ce6-b5d1-512cdf3314d4",
   "metadata": {},
   "source": [
    "# PHáº¦N 5: Cáº¬P NHáº¬T CÃ‚U Há»I â€“ CÃ‚U TRáº¢ Lá»œI VÃ€O FILE CSV (QA DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95276012-48ec-450c-af38-567befe2f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_DB_FILE = \"qa_database.csv\"\n",
    "\n",
    "def load_qa_database(filename):\n",
    "    \"\"\"Äá»c dá»¯ liá»‡u tá»« file CSV náº¿u tá»“n táº¡i.\"\"\"\n",
    "    data = []\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, mode='r', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                data.append(row)\n",
    "    return data\n",
    "\n",
    "def save_qa_database(filename, data):\n",
    "    \"\"\"Ghi danh sÃ¡ch cÃ¡c cÃ¢u há»i â€“ cÃ¢u tráº£ lá»i vÃ o file CSV.\"\"\"\n",
    "    fieldnames = ['question', 'answer', 'created_at', 'updated_at']\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def update_qa_database(filename, new_question, new_answer):\n",
    "    \"\"\"\n",
    "    Náº¿u cÃ¢u há»i Ä‘Ã£ cÃ³ (so sÃ¡nh khÃ´ng phÃ¢n biá»‡t hoa thÆ°á»ng), cáº­p nháº­t cÃ¢u tráº£ lá»i vÃ  thá»i gian;\n",
    "    Náº¿u chÆ°a cÃ³ thÃ¬ thÃªm má»›i.\n",
    "    \"\"\"\n",
    "    data = load_qa_database(filename)\n",
    "    found = False\n",
    "    for entry in data:\n",
    "        if entry['question'].strip().lower() == new_question.strip().lower():\n",
    "            entry['answer'] = new_answer\n",
    "            entry['updated_at'] = datetime.now().isoformat()\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        now_str = datetime.now().isoformat()\n",
    "        data.append({\n",
    "            'question': new_question,\n",
    "            'answer': new_answer,\n",
    "            'created_at': now_str,\n",
    "            'updated_at': now_str\n",
    "        })\n",
    "    save_qa_database(filename, data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd65c8-8009-41e1-9570-6203215f9c65",
   "metadata": {},
   "source": [
    "# PHáº¦N 6: TRUY XUáº¤T THÃ”NG TIN Äáº¦Y Äá»¦ Tá»ª VÄ‚N Báº¢N PHÃP LUáº¬T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7d4e7-012c-410b-8378-9bb9ba12f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_law_info(query, doc_threshold=0.5, top_k=3):\n",
    "    \"\"\"\n",
    "    Nháº­n cÃ¢u há»i tá»« ngÆ°á»i dÃ¹ng, chuyá»ƒn Ä‘á»•i thÃ nh embedding vÃ  tÃ­nh toÃ¡n\n",
    "    cosine similarity vá»›i cÃ¡c Ä‘oáº¡n vÄƒn báº£n phÃ¡p luáº­t. Sau Ä‘Ã³, láº¥y top_k Ä‘oáº¡n\n",
    "    cÃ³ Ä‘iá»ƒm cao nhÆ°ng ghÃ©p chÃºng láº¡i thÃ nh _má»™t_ cÃ¢u tráº£ lá»i duy nháº¥t.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_model.encode(query, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, paragraph_embeddings)[0]\n",
    "    top_scores, top_indices = torch.topk(cosine_scores, k=top_k)\n",
    "    \n",
    "    # Lá»c cÃ¡c Ä‘oáº¡n cÃ³ Ä‘iá»ƒm cao vÆ°á»£t ngÆ°á»¡ng\n",
    "    filtered_paragraphs = [paragraphs[idx] for score, idx in zip(top_scores, top_indices) if score.item() >= doc_threshold]\n",
    "    \n",
    "    if not filtered_paragraphs:\n",
    "        return \"âš ï¸ Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p trong cÃ¡c vÄƒn báº£n phÃ¡p luáº­t.\"\n",
    "\n",
    "    # GhÃ©p láº¡i cÃ¡c Ä‘oáº¡n Ä‘Ã£ lá»c thÃ nh má»™t cÃ¢u tráº£ lá»i duy nháº¥t\n",
    "    aggregated_answer = \"\\n\\n\".join(filtered_paragraphs)\n",
    "    final_answer = \"ğŸ“œ [ThÃ´ng tin tá»« vÄƒn báº£n phÃ¡p luáº­t]\\n\" + aggregated_answer\n",
    "\n",
    "    # Cáº­p nháº­t vÃ o Q&A database\n",
    "    update_qa_database(QA_DB_FILE, query, final_answer)\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b0ef9a-807a-4fad-9ab5-c8545e9399aa",
   "metadata": {},
   "source": [
    "# PHáº¦N 7: GIAO DIá»†N CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd28e8b-90d3-4613-b9df-28411847f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_loop():\n",
    "    print(\"\\nğŸ¤– ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i Chatbot tÆ° váº¥n phÃ¡p luáº­t!\")\n",
    "    print(\"GÃµ 'exit' hoáº·c 'thoÃ¡t' Ä‘á»ƒ káº¿t thÃºc cuá»™c trÃ² chuyá»‡n.\")\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(\"\\nğŸ“ Báº¡n: \").strip()\n",
    "        if user_query.lower() in ['exit', 'thoÃ¡t']:\n",
    "            print(\"ğŸ‘‹ Chatbot: Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng dá»‹ch vá»¥!\")\n",
    "            break\n",
    "        # Láº¥y ra _má»™t_ cÃ¢u tráº£ lá»i duy nháº¥t, Ä‘áº§y Ä‘á»§ vÃ  ghÃ©p tá»« cÃ¡c Ä‘oáº¡n liÃªn quan\n",
    "        answer = retrieve_law_info(user_query)\n",
    "        print(f\"\\nğŸ’¡ Chatbot:\\n{answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58689d-5744-4e90-901f-b00b1373276e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67455d7-2ab7-4275-ba26-afbed551bc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
