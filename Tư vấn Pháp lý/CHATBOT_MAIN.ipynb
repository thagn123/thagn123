{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adebdd3d-8f4a-4c37-85fa-1872f6a4face",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2docx\n",
      "  Downloading pdf2docx-0.5.8-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: python-docx in c:\\users\\daoth\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\daoth\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\daoth\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\daoth\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\daoth\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Collecting PyMuPDF>=1.19.0 (from pdf2docx)\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: fonttools>=4.24.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pdf2docx) (4.51.0)\n",
      "Collecting opencv-python-headless>=4.5 (from pdf2docx)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting fire>=0.3.0 (from pdf2docx)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from python-docx) (4.11.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Collecting termcolor (from fire>=0.3.0->pdf2docx)\n",
      "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daoth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Downloading pdf2docx-0.5.8-py3-none-any.whl (132 kB)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "   ---------------------------------------- 0.0/39.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/39.4 MB 7.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.1/39.4 MB 5.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.9/39.4 MB 4.8 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 4.2/39.4 MB 5.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 5.2/39.4 MB 5.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.6/39.4 MB 5.4 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 8.1/39.4 MB 5.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 9.4/39.4 MB 5.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 11.0/39.4 MB 6.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 12.1/39.4 MB 5.9 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 13.4/39.4 MB 5.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 14.7/39.4 MB 5.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 16.0/39.4 MB 6.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 16.5/39.4 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 17.8/39.4 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 19.1/39.4 MB 5.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 20.2/39.4 MB 5.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 21.2/39.4 MB 5.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 22.3/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 23.6/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 24.9/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 26.0/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 27.0/39.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 28.0/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 29.4/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 30.4/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 31.7/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 33.0/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 34.1/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 35.4/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.7/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.7/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.4/39.4 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/16.6 MB 6.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.4/16.6 MB 5.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/16.6 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.0/16.6 MB 5.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 6.0/16.6 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.3/16.6 MB 5.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.7/16.6 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.7/16.6 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.7/16.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.1/16.6 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.4/16.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.4/16.6 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.0/16.6 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114262 sha256=7e68845a37ef53ab63fe7f338e3c04fa89969962f19a9e1c6db57f7897e3f51a\n",
      "  Stored in directory: c:\\users\\daoth\\appdata\\local\\pip\\cache\\wheels\\9e\\5b\\45\\29f72e55d87a29426b04b3cfdf20325c079eb97ab74f59017d\n",
      "Successfully built fire\n",
      "Installing collected packages: termcolor, PyMuPDF, opencv-python-headless, fire, pdf2docx\n",
      "Successfully installed PyMuPDF-1.25.5 fire-0.7.0 opencv-python-headless-4.11.0.86 pdf2docx-0.5.8 termcolor-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pdf2docx PyPDF2 python-docx sentence-transformers numpy scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f5537-affb-446c-b9dd-296fb89c9b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2fa90-db94-48a1-9ef9-4061a3a22d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "from pdf2docx import Converter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f0c9f-4fa5-4b6c-94ca-bbbe8a438290",
   "metadata": {},
   "source": [
    "# PHẦN 1: CHUYỂN ĐỔI VÀ TRÍCH XUẤT VĂN BẢN TỪ THƯ MỤC data_cawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c00a7-dba2-4e09-9989-6837b9f30639",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data_crawl\"\n",
    "EXTRACTED_TEXT_FILE = os.path.join(DATA_FOLDER, \"extracted_data.txt\")\n",
    "\n",
    "def convert_pdf_to_docx(pdf_path, docx_path):\n",
    "    \"\"\"Chuyển đổi file PDF sang DOCX để trích xuất nội dung.\"\"\"\n",
    "    try:\n",
    "        cv = Converter(pdf_path)\n",
    "        cv.convert(docx_path, start=0, end=None)\n",
    "        cv.close()\n",
    "        print(f\"✅ Đã chuyển đổi PDF sang DOCX: {docx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi chuyển đổi PDF {pdf_path}: {e}\")\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    \"\"\"Trích xuất nội dung từ file DOCX.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        for para in doc.paragraphs:\n",
    "            if para.text.strip():\n",
    "                text += para.text.strip() + \"\\n\"\n",
    "        print(f\"✅ Đã trích xuất nội dung từ: {docx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi đọc DOCX {docx_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def process_all_documents(folder_path):\n",
    "    \"\"\"\n",
    "    Duyệt qua thư mục data_cawl, chuyển PDF sang DOCX (nếu cần)\n",
    "    và trích xuất nội dung từ các file DOCX.\n",
    "    \"\"\"\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Nếu là PDF thì chuyển sang DOCX trước\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            docx_path = file_path[:-4] + \".docx\"  # thay .pdf thành .docx\n",
    "            convert_pdf_to_docx(file_path, docx_path)\n",
    "            all_texts.append(extract_text_from_docx(docx_path))\n",
    "        elif filename.lower().endswith(\".docx\"):\n",
    "            all_texts.append(extract_text_from_docx(file_path))\n",
    "    return \"\\n\".join(all_texts)\n",
    "\n",
    "def save_extracted_text(text, filename=EXTRACTED_TEXT_FILE):\n",
    "    \"\"\"Lưu văn bản đã trích xuất vào file TXT.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"✅ Đã lưu văn bản trích xuất vào: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi lưu văn bản: {e}\")\n",
    "\n",
    "def load_extracted_text(filename=EXTRACTED_TEXT_FILE):\n",
    "    \"\"\"Tải văn bản đã trích xuất từ file TXT.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        print(f\"✅ Đã tải nội dung từ: {filename}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi tải văn bản từ file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Nếu file văn bản đã có, tải trực tiếp từ file; nếu không, xử lý gốc và lưu lại.\n",
    "if os.path.exists(EXTRACTED_TEXT_FILE):\n",
    "    combined_text = load_extracted_text()\n",
    "else:\n",
    "    combined_text = process_all_documents(DATA_FOLDER)\n",
    "    save_extracted_text(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdb443-f02f-4a0f-aef4-937a47679042",
   "metadata": {},
   "source": [
    "# PHẦN 2: TIỀN XỬ LÝ VĂN BẢN – CHIA THÀNH CÁC ĐOẠN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87698794-6128-4cf2-99d6-9f71673e74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_paragraphs(text, min_length=50):\n",
    "    \"\"\"Chia văn bản thành các đoạn có ý nghĩa, bỏ qua các đoạn quá ngắn.\"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\") if len(p.strip()) >= min_length]\n",
    "    return paragraphs\n",
    "\n",
    "paragraphs = split_into_paragraphs(combined_text)\n",
    "print(f\"📜 Số đoạn văn bản pháp luật trích xuất: {len(paragraphs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d4583-9318-4cec-a1de-5317b66d9aa3",
   "metadata": {},
   "source": [
    "# PHẦN 3: TẠO EMBEDDINGS CHO CÁC ĐOẠN VĂN BẢN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71d509-59fd-4fb7-bb33-a2fe1d97ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "print(\"\\n🔎 Đang tạo embeddings cho văn bản pháp luật (quá trình có thể mất vài giây)...\")\n",
    "paragraph_embeddings = embed_model.encode(paragraphs, convert_to_tensor=True)\n",
    "print(\"✅ Hoàn thành việc tạo embeddings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c472dbcf-95ef-48fb-81c0-9ac356264d43",
   "metadata": {},
   "source": [
    "# PHẦN 4: TRÍCH XUẤT THÔNG TIN 'ĐIỀU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b0b98-50ff-4ca9-9992-56f513744b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_info(text):\n",
    "    \"\"\"\n",
    "    Sử dụng regex để tìm kiếm mẫu như \"Điều 15\" hoặc \"Điều15.1\" trong đoạn văn.\n",
    "    Trả về thông tin 'Điều' nếu tìm thấy, ngược lại trả về None.\n",
    "    \"\"\"\n",
    "    pattern = r'Điều\\s*\\d+([.,]\\d+)?'\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64178d2-7429-4ce6-b5d1-512cdf3314d4",
   "metadata": {},
   "source": [
    "# PHẦN 5: CẬP NHẬT CÂU HỎI – CÂU TRẢ LỜI VÀO FILE CSV (QA DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95276012-48ec-450c-af38-567befe2f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_DB_FILE = \"qa_database.csv\"\n",
    "\n",
    "def load_qa_database(filename):\n",
    "    \"\"\"Đọc dữ liệu từ file CSV nếu tồn tại.\"\"\"\n",
    "    data = []\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, mode='r', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                data.append(row)\n",
    "    return data\n",
    "\n",
    "def save_qa_database(filename, data):\n",
    "    \"\"\"Ghi danh sách các câu hỏi – câu trả lời vào file CSV.\"\"\"\n",
    "    fieldnames = ['question', 'answer', 'created_at', 'updated_at']\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def update_qa_database(filename, new_question, new_answer):\n",
    "    \"\"\"\n",
    "    Nếu câu hỏi đã có (so sánh không phân biệt hoa thường), cập nhật câu trả lời và thời gian;\n",
    "    Nếu chưa có thì thêm mới.\n",
    "    \"\"\"\n",
    "    data = load_qa_database(filename)\n",
    "    found = False\n",
    "    for entry in data:\n",
    "        if entry['question'].strip().lower() == new_question.strip().lower():\n",
    "            entry['answer'] = new_answer\n",
    "            entry['updated_at'] = datetime.now().isoformat()\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        now_str = datetime.now().isoformat()\n",
    "        data.append({\n",
    "            'question': new_question,\n",
    "            'answer': new_answer,\n",
    "            'created_at': now_str,\n",
    "            'updated_at': now_str\n",
    "        })\n",
    "    save_qa_database(filename, data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd65c8-8009-41e1-9570-6203215f9c65",
   "metadata": {},
   "source": [
    "# PHẦN 6: TRUY XUẤT THÔNG TIN ĐẦY ĐỦ TỪ VĂN BẢN PHÁP LUẬT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7d4e7-012c-410b-8378-9bb9ba12f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_law_info(query, doc_threshold=0.5, top_k=3):\n",
    "    \"\"\"\n",
    "    Nhận câu hỏi từ người dùng, chuyển đổi thành embedding và tính toán\n",
    "    cosine similarity với các đoạn văn bản pháp luật. Sau đó, lấy top_k đoạn\n",
    "    có điểm cao nhưng ghép chúng lại thành _một_ câu trả lời duy nhất.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_model.encode(query, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, paragraph_embeddings)[0]\n",
    "    top_scores, top_indices = torch.topk(cosine_scores, k=top_k)\n",
    "    \n",
    "    # Lọc các đoạn có điểm cao vượt ngưỡng\n",
    "    filtered_paragraphs = [paragraphs[idx] for score, idx in zip(top_scores, top_indices) if score.item() >= doc_threshold]\n",
    "    \n",
    "    if not filtered_paragraphs:\n",
    "        return \"⚠️ Xin lỗi, tôi không tìm thấy thông tin phù hợp trong các văn bản pháp luật.\"\n",
    "\n",
    "    # Ghép lại các đoạn đã lọc thành một câu trả lời duy nhất\n",
    "    aggregated_answer = \"\\n\\n\".join(filtered_paragraphs)\n",
    "    final_answer = \"📜 [Thông tin từ văn bản pháp luật]\\n\" + aggregated_answer\n",
    "\n",
    "    # Cập nhật vào Q&A database\n",
    "    update_qa_database(QA_DB_FILE, query, final_answer)\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b0ef9a-807a-4fad-9ab5-c8545e9399aa",
   "metadata": {},
   "source": [
    "# PHẦN 7: GIAO DIỆN CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd28e8b-90d3-4613-b9df-28411847f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_loop():\n",
    "    print(\"\\n🤖 Chào mừng bạn đến với Chatbot tư vấn pháp luật!\")\n",
    "    print(\"Gõ 'exit' hoặc 'thoát' để kết thúc cuộc trò chuyện.\")\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(\"\\n📝 Bạn: \").strip()\n",
    "        if user_query.lower() in ['exit', 'thoát']:\n",
    "            print(\"👋 Chatbot: Cảm ơn bạn đã sử dụng dịch vụ!\")\n",
    "            break\n",
    "        # Lấy ra _một_ câu trả lời duy nhất, đầy đủ và ghép từ các đoạn liên quan\n",
    "        answer = retrieve_law_info(user_query)\n",
    "        print(f\"\\n💡 Chatbot:\\n{answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58689d-5744-4e90-901f-b00b1373276e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67455d7-2ab7-4275-ba26-afbed551bc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
